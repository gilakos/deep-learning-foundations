{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "In this notebook, we'll be building a generative adversarial network (GAN) trained on the MNIST dataset. From this, we'll be able to generate new handwritten digits!\n",
    "\n",
    "GANs were [first reported on](https://arxiv.org/abs/1406.2661) in 2014 from Ian Goodfellow and others in Yoshua Bengio's lab. Since then, GANs have exploded in popularity. Here are a few examples to check out:\n",
    "\n",
    "* [Pix2Pix](https://affinelayer.com/pixsrv/) \n",
    "* [CycleGAN](https://github.com/junyanz/CycleGAN)\n",
    "* [A whole list](https://github.com/wiseodd/generative-models)\n",
    "\n",
    "The idea behind GANs is that you have two networks, a generator $G$ and a discriminator $D$, competing against each other. The generator makes fake data to pass to the discriminator. The discriminator also sees real data and predicts if the data it's received is real or fake. The generator is trained to fool the discriminator, it wants to output data that looks _as close as possible_ to real data. And the discriminator is trained to figure out which data is real and which is fake. What ends up happening is that the generator learns to make data that is indistiguishable from real data to the discriminator.\n",
    "\n",
    "![GAN diagram](assets/gan_diagram.png)\n",
    "\n",
    "The general structure of a GAN is shown in the diagram above, using MNIST images as data. The latent sample is a random vector the generator uses to contruct it's fake images. As the generator learns through training, it figures out how to map these random vectors to recognizable images that can foold the discriminator.\n",
    "\n",
    "The output of the discriminator is a sigmoid function, where 0 indicates a fake image and 1 indicates an real image. If you're interested only in generating new images, you can throw out the discriminator after training. Now, let's see how we build this thing in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Inputs\n",
    "\n",
    "First we need to create the inputs for our graph. We need two inputs, one for the discriminator and one for the generator. Here we'll call the discriminator input `inputs_real` and the generator input `inputs_z`. We'll assign them the appropriate sizes for each of the networks.\n",
    "\n",
    ">**Exercise:** Finish the `model_inputs` function below. Create the placeholders for `inputs_real` and `inputs_z` using the input sizes `real_dim` and `z_dim` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    inputs_real = tf.placeholder(tf.float32, (None, real_dim), name='inputs_real')\n",
    "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='inputs_z')\n",
    "    \n",
    "    return inputs_real, inputs_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generator network\n",
    "\n",
    "![GAN Network](assets/gan_network.png)\n",
    "\n",
    "Here we'll build the generator network. To make this network a universal function approximator, we'll need at least one hidden layer. We should use a leaky ReLU to allow gradients to flow backwards through the layer unimpeded. A leaky ReLU is like a normal ReLU, except that there is a small non-zero output for negative input values.\n",
    "\n",
    "#### Variable Scope\n",
    "Here we need to use `tf.variable_scope` for two reasons. Firstly, we're going to make sure all the variable names start with `generator`. Similarly, we'll prepend `discriminator` to the discriminator variables. This will help out later when we're training the separate networks.\n",
    "\n",
    "We could just use `tf.name_scope` to set the names, but we also want to reuse these networks with different inputs. For the generator, we're going to train it, but also _sample from it_ as we're training and after training. The discriminator will need to share variables between the fake and real input images. So, we can use the `reuse` keyword for `tf.variable_scope` to tell TensorFlow to reuse the variables instead of creating new ones if we build the graph again.\n",
    "\n",
    "To use `tf.variable_scope`, you use a `with` statement:\n",
    "```python\n",
    "with tf.variable_scope('scope_name', reuse=False):\n",
    "    # code here\n",
    "```\n",
    "\n",
    "Here's more from [the TensorFlow documentation](https://www.tensorflow.org/programmers_guide/variable_scope#the_problem) to get another look at using `tf.variable_scope`.\n",
    "\n",
    "#### Leaky ReLU\n",
    "TensorFlow doesn't provide an operation for leaky ReLUs, so we'll need to make one . For this you can use take the outputs from a linear fully connected layer and pass them to `tf.maximum`. Typically, a parameter `alpha` sets the magnitude of the output for negative values. So, the output for negative input (`x`) values is `alpha*x`, and the output for positive `x` is `x`:\n",
    "$$\n",
    "f(x) = max(\\alpha * x, x)\n",
    "$$\n",
    "\n",
    "#### Tanh Output\n",
    "The generator has been found to perform the best with $tanh$ for the generator output. This means that we'll have to rescale the MNIST images to be between -1 and 1, instead of 0 and 1.\n",
    "\n",
    ">**Exercise:** Implement the generator network in the function below. You'll need to return the tanh output. Make sure to wrap your code in a variable scope, with 'generator' as the scope name, and pass the `reuse` keyword argument from the function to `tf.variable_scope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generator(z, out_dim, n_units=128, reuse=False,  alpha=0.01):\n",
    "    ''' Build the generator network.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        z : Input tensor for the generator\n",
    "        out_dim : Shape of the generator output\n",
    "        n_units : Number of units in hidden layer\n",
    "        reuse : Reuse the variables with tf.variable_scope\n",
    "        alpha : leak parameter for leaky ReLU\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out, logits: \n",
    "    '''\n",
    "    with tf.variable_scope ('generator', reuse=False): # finish this\n",
    "        # Hidden layer\n",
    "        h1 = tf.layers.dense(z, n_units, activation=None)\n",
    "        # Leaky ReLU\n",
    "        h1 = tf.maximum(alpha*h1, h1)\n",
    "        \n",
    "        # Logits and tanh output\n",
    "        logits = tf.layers.dense(h1, out_dim, activation=None)\n",
    "        out = tf.tanh(logits)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Discriminator\n",
    "\n",
    "The discriminator network is almost exactly the same as the generator network, except that we're using a sigmoid output layer.\n",
    "\n",
    ">**Exercise:** Implement the discriminator network in the function below. Same as above, you'll need to return both the logits and the sigmoid output. Make sure to wrap your code in a variable scope, with 'discriminator' as the scope name, and pass the `reuse` keyword argument from the function arguments to `tf.variable_scope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n",
    "    ''' Build the discriminator network.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x : Input tensor for the discriminator\n",
    "        n_units: Number of units in hidden layer\n",
    "        reuse : Reuse the variables with tf.variable_scope\n",
    "        alpha : leak parameter for leaky ReLU\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out, logits: \n",
    "    '''\n",
    "    with tf.variable_scope ('discriminator', reuse=reuse): # finish this\n",
    "        # Hidden layer\n",
    "        h1 = tf.layers.dense(x, n_units, activation=None)\n",
    "        # Leaky ReLU\n",
    "        h1 = tf.maximum(alpha*h1, h1)\n",
    "        \n",
    "        logits = tf.layers.dense(h1, 1, activation=None)\n",
    "        out = tf.sigmoid(logits)\n",
    "        \n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Size of input image to discriminator\n",
    "input_size = 784 # 28x28 MNIST images flattened\n",
    "# Size of latent vector to generator\n",
    "z_size = 100\n",
    "# Sizes of hidden layers in generator and discriminator\n",
    "g_hidden_size = 128\n",
    "d_hidden_size = 128\n",
    "# Leak factor for leaky ReLU\n",
    "alpha = 0.01\n",
    "# Label smoothing \n",
    "smooth = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build network\n",
    "\n",
    "Now we're building the network from the functions defined above.\n",
    "\n",
    "First is to get our inputs, `input_real, input_z` from `model_inputs` using the sizes of the input and z.\n",
    "\n",
    "Then, we'll create the generator, `generator(input_z, input_size)`. This builds the generator with the appropriate input and output sizes.\n",
    "\n",
    "Then the discriminators. We'll build two of them, one for real data and one for fake data. Since we want the weights to be the same for both real and fake data, we need to reuse the variables. For the fake data, we're getting it from the generator as `g_model`. So the real data discriminator is `discriminator(input_real)` while the fake discriminator is `discriminator(g_model, reuse=True)`.\n",
    "\n",
    ">**Exercise:** Build the network from the functions you defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Create our input placeholders\n",
    "input_real, input_z = model_inputs(input_size, z_size)\n",
    "\n",
    "# Generator network here\n",
    "g_model = generator(input_z, input_size)\n",
    "# g_model is the generator output\n",
    "\n",
    "# Disriminator network here\n",
    "d_model_real, d_logits_real = discriminator(input_real)\n",
    "d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Discriminator and Generator Losses\n",
    "\n",
    "Now we need to calculate the losses, which is a little tricky. For the discriminator, the total loss is the sum of the losses for real and fake images, `d_loss = d_loss_real + d_loss_fake`. The losses will by sigmoid cross-entropys, which we can get with `tf.nn.sigmoid_cross_entropy_with_logits`. We'll also wrap that in `tf.reduce_mean` to get the mean for all the images in the batch. So the losses will look something like \n",
    "\n",
    "```python\n",
    "tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "```\n",
    "\n",
    "For the real image logits, we'll use `d_logits_real` which we got from the discriminator in the cell above. For the labels, we want them to be all ones, since these are all real images. To help the discriminator generalize better, the labels are reduced a bit from 1.0 to 0.9, for example,  using the parameter `smooth`. This is known as label smoothing, typically used with classifiers to improve performance. In TensorFlow, it looks something like `labels = tf.ones_like(tensor) * (1 - smooth)`\n",
    "\n",
    "The discriminator loss for the fake data is similar. The logits are `d_logits_fake`, which we got from passing the generator output to the discriminator. These fake logits are used with labels of all zeros. Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.\n",
    "\n",
    "Finally, the generator losses are using `d_logits_fake`, the fake image logits. But, now the labels are all ones. The generator is trying to fool the discriminator, so it wants to discriminator to output ones for fake images.\n",
    "\n",
    ">**Exercise:** Calculate the losses for the discriminator and the generator. There are two discriminator losses, one for real images and one for fake images. For the real image loss, use the real logits and (smoothed) labels of ones. For the fake image loss, use the fake logits with labels of all zeros. The total discriminator loss is the sum of those two losses. Finally, the generator loss again uses the fake logits from the discriminator, but this time the labels are all ones because the generator wants to fool the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Calculate losses\n",
    "d_loss_real = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, \n",
    "                                                          labels=tf.ones_like(d_logits_real) * (1 - smooth)))\n",
    "d_loss_fake = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, \n",
    "                                                          labels=tf.zeros_like(d_logits_real)))\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "g_loss = tf.reduce_mean(\n",
    "             tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                     labels=tf.ones_like(d_logits_fake)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimizers\n",
    "\n",
    "We want to update the generator and discriminator variables separately. So we need to get the variables for each part build optimizers for the two parts. To get all the trainable variables, we use `tf.trainable_variables()`. This creates a list of all the variables we've defined in our graph.\n",
    "\n",
    "For the generator optimizer, we only want to generator variables. Our past selves were nice and used a variable scope to start all of our generator variable names with `generator`. So, we just need to iterate through the list from `tf.trainable_variables()` and keep variables to start with `generator`. Each variable object has an attribute `name` which holds the name of the variable as a string (`var.name == 'weights_0'` for instance). \n",
    "\n",
    "We can do something similar with the discriminator. All the variables in the discriminator start with `discriminator`.\n",
    "\n",
    "Then, in the optimizer we pass the variable lists to `var_list` in the `minimize` method. This tells the optimizer to only update the listed variables. Something like `tf.train.AdamOptimizer().minimize(loss, var_list=var_list)` will only train the variables in `var_list`.\n",
    "\n",
    ">**Exercise: ** Below, implement the optimizers for the generator and discriminator. First you'll need to get a list of trainable variables, then split that list into two lists, one for the generator variables and another for the discriminator variables. Finally, using `AdamOptimizer`, create an optimizer for each network that update the network variables separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable generator/dense/kernel/Adam/ already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-16-ce26135605fb>\", line 10, in <module>\n    g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n  File \"/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ce26135605fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0md_train_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mg_train_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[0;32m--> 298\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   def compute_gradients(self, loss, var_list=None,\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    410\u001b[0m                        ([str(v) for _, _, v in converted_grads_and_vars],))\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/training/adam.py\u001b[0m in \u001b[0;36m_create_slots\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# Create slots for the first and second moments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_zeros_slot\u001b[0;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m       \u001b[0mnamed_slots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslot_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[0;34m(primary, name, dtype, colocate_with_primary)\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   return create_slot(primary, val, name,\n\u001b[0;32m--> 123\u001b[0;31m                      colocate_with_primary=colocate_with_primary)\n\u001b[0m",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_slot\u001b[0;34m(primary, val, name, colocate_with_primary)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolocate_with_primary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_create_slot_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_create_slot_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36m_create_slot_var\u001b[0;34m(primary, val, scope)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0mcurrent_partitioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0mslot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_partitioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    346\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    637\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 639\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    640\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable generator/dense/kernel/Adam/ already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-16-ce26135605fb>\", line 10, in <module>\n    g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n  File \"/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# Optimizers\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Get the trainable_variables, split into G and D parts\n",
    "t_vars = tf.trainable_variables()\n",
    "g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100... Discriminator Loss: 0.3514... Generator Loss: 4.0014\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable generator/dense/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-8-19cd38e6adfb>\", line 18, in generator\n    h1 = tf.layers.dense(z, n_units, activation=None)\n  File \"<ipython-input-14-a0bf95588b68>\", line 6, in <module>\n    g_model = generator(input_z, input_size)\n  File \"/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c104e1205ecd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0msample_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         gen_samples = sess.run(\n\u001b[0;32m---> 36\u001b[0;31m                        \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                        feed_dict={input_z: sample_z})\n\u001b[1;32m     38\u001b[0m         \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-19cd38e6adfb>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(z, out_dim, n_units, reuse, alpha)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'generator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# finish this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Leaky ReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 _reuse=reuse)\n\u001b[0;32m--> 216\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    121\u001b[0m                                   \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                                   \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                                   trainable=True)\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m       self.bias = vs.get_variable('bias',\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mvariable_getter\u001b[0;34m(getter, name, shape, dtype, initializer, regularizer, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m           variable_getter=functools.partial(getter, **kwargs))\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# Build (if necessary) and call the layer, inside a variable scope.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m_add_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, variable_getter)\u001b[0m\n\u001b[1;32m    206\u001b[0m                                \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                                \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                                trainable=trainable and self.trainable)\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;31m# TODO(sguada) fix name = variable.op.name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    637\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 639\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    640\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable generator/dense/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-8-19cd38e6adfb>\", line 18, in generator\n    h1 = tf.layers.dense(z, n_units, activation=None)\n  File \"<ipython-input-14-a0bf95588b68>\", line 6, in <module>\n    g_model = generator(input_z, input_size)\n  File \"/Users/gilakos/anaconda/envs/dlp3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "samples = []\n",
    "losses = []\n",
    "saver = tf.train.Saver(var_list = g_vars)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for ii in range(mnist.train.num_examples//batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Get images, reshape and rescale to pass to D\n",
    "            batch_images = batch[0].reshape((batch_size, 784))\n",
    "            batch_images = batch_images*2 - 1\n",
    "            \n",
    "            # Sample random noise for G\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            \n",
    "            # Run optimizers\n",
    "            _ = sess.run(d_train_opt, feed_dict={input_real: batch_images, input_z: batch_z})\n",
    "            _ = sess.run(g_train_opt, feed_dict={input_z: batch_z})\n",
    "        \n",
    "        # At the end of each epoch, get the losses and print them out\n",
    "        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_real: batch_images})\n",
    "        train_loss_g = g_loss.eval({input_z: batch_z})\n",
    "            \n",
    "        print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "              \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "              \"Generator Loss: {:.4f}\".format(train_loss_g))    \n",
    "        # Save losses to view after training\n",
    "        losses.append((train_loss_d, train_loss_g))\n",
    "        \n",
    "        # Sample from generator as we're training for viewing afterwards\n",
    "        sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "        gen_samples = sess.run(\n",
    "                       generator(input_z, input_size, reuse=True),\n",
    "                       feed_dict={input_z: sample_z})\n",
    "        samples.append(gen_samples)\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n",
    "\n",
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(samples, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training loss\n",
    "\n",
    "Here we'll check out the training losses for the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x123583cc0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1235a72b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator')\n",
    "plt.plot(losses.T[1], label='Generator')\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generator samples from training\n",
    "\n",
    "Here we can view samples of images from the generator. First we'll look at images taken while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def view_samples(epoch, samples):\n",
    "    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load samples from generator taken while training\n",
    "with open('train_samples.pkl', 'rb') as f:\n",
    "    samples = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "These are samples from the final training epoch. You can see the generator is able to reproduce numbers like 5, 7, 3, 0, 9. Since this is just a sample, it isn't representative of the full range of images this generator can make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_ = view_samples(-1, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below I'm showing the generated images as the network was training, every 10 epochs. With bonus optical illusion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rows, cols = 10, 6\n",
    "fig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n",
    "\n",
    "for sample, ax_row in zip(samples[::int(len(samples)/rows)], axes):\n",
    "    for img, ax in zip(sample[::int(len(sample)/cols)], ax_row):\n",
    "        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It starts out as all noise. Then it learns to make only the center white and the rest black. You can start to see some number like structures appear out of the noise. Looks like 1, 9, and 8 show up first. Then, it learns 5 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling from the generator\n",
    "\n",
    "We can also get completely new images from the generator by using the checkpoint we saved after training. We just need to pass in a new latent vector $z$ and we'll get new samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(var_list=g_vars)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "    gen_samples = sess.run(\n",
    "                   generator(input_z, input_size, reuse=True),\n",
    "                   feed_dict={input_z: sample_z})\n",
    "view_samples(0, [gen_samples])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
